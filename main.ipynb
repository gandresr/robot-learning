{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: could not load d4rl environments!\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "import pickle\n",
    "import robomimic.utils.torch_utils as TorchUtils\n",
    "import robomimic.utils.train_utils as TrainUtils\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "\n",
    "from utils.environment import *\n",
    "from utils.dataset import *\n",
    "from utils.functions import *\n",
    "from utils.rollout import *\n",
    "from utils.test import *\n",
    "from typing_extensions import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "from robomimic.config import config_factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Configuration File\n",
    "\n",
    "We start from a vanilla behavioral cloning (BC) algorithm, and generate an ensemble of NNs to extract the data that maximizes the performance of the BC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment and type of demonstration data\n",
    "env_fname = 'can' # ('can', 'lift')\n",
    "if env_fname == 'can':\n",
    "    envname = 'PickPlaceCan'\n",
    "elif env_fname == 'lift':\n",
    "    envname = 'Lift'\n",
    "# Types of demonstration data from Robomimic (ph: proficient human, mh: multi-human)\n",
    "demo_type = 'ph' # ('ph', 'mh')\n",
    "# Define dataset path\n",
    "dataset_path = f\"datasets/{env_fname}/{demo_type}/low_dim.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "obs modality object with shape (14,)\n",
      "obs modality robot0_eef_pos with shape (3,)\n",
      "obs modality robot0_eef_quat with shape (4,)\n",
      "obs modality robot0_gripper_qpos with shape (2,)\n"
     ]
    }
   ],
   "source": [
    "config = config_factory(algo_name=\"bc\")\n",
    "ObsUtils.initialize_obs_utils_with_config(config)\n",
    "device = TorchUtils.get_torch_device(try_to_use_cuda=True) # get torch device\n",
    "eval_epoch = 10\n",
    "\n",
    "# Load environment's metadata\n",
    "env_meta = FileUtils.get_env_metadata_from_dataset(dataset_path=dataset_path)\n",
    "shape_meta = FileUtils.get_shape_metadata_from_dataset(\n",
    "    dataset_path = dataset_path,\n",
    "    all_modalities = config.observation.modalities.obs.low_dim,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('object', (14,)),\n",
       "             ('robot0_eef_pos', (3,)),\n",
       "             ('robot0_eef_quat', (4,)),\n",
       "             ('robot0_gripper_qpos', (2,))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_meta['all_shapes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING\n"
     ]
    }
   ],
   "source": [
    "envs = OrderedDict()\n",
    "env_names = [env_meta[\"env_name\"]]\n",
    "for env_name in env_names:\n",
    "    envs[env_name] = \\\n",
    "        create_env_from_metadata(env_meta=env_meta, env_name=env_name)\n",
    "log_dir, ckpt_dir, video_dir = TrainUtils.get_exp_dir(config) # paths to save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Demonstration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['demo_0', 'demo_1', 'demo_2', 'demo_3', 'demo_4', 'demo_5', 'demo_6', 'demo_7', 'demo_8', 'demo_9', 'demo_10', 'demo_11', 'demo_12', 'demo_13', 'demo_14', 'demo_15', 'demo_16', 'demo_17', 'demo_18', 'demo_19', 'demo_20', 'demo_21', 'demo_22', 'demo_23', 'demo_24', 'demo_25', 'demo_26', 'demo_27', 'demo_28', 'demo_29', 'demo_30', 'demo_31', 'demo_32', 'demo_33', 'demo_34', 'demo_35', 'demo_36', 'demo_37', 'demo_38', 'demo_39', 'demo_40', 'demo_41', 'demo_42', 'demo_43', 'demo_44', 'demo_45', 'demo_46', 'demo_47', 'demo_48', 'demo_49', 'demo_50', 'demo_51', 'demo_52', 'demo_53', 'demo_54', 'demo_55', 'demo_56', 'demo_57', 'demo_58', 'demo_59', 'demo_60', 'demo_61', 'demo_62', 'demo_63', 'demo_64', 'demo_65', 'demo_66', 'demo_67', 'demo_68', 'demo_69', 'demo_70', 'demo_71', 'demo_72', 'demo_73', 'demo_74', 'demo_75', 'demo_76', 'demo_77', 'demo_78', 'demo_79', 'demo_80', 'demo_81', 'demo_82', 'demo_83', 'demo_84', 'demo_85', 'demo_86', 'demo_87', 'demo_88', 'demo_89']\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 90/90 [00:00<00:00, 396.53it/s]\n",
      "SequenceDataset: caching get_item calls...\n",
      "100%|██████████| 10199/10199 [00:01<00:00, 8630.57it/s]\n",
      "['demo_90', 'demo_91', 'demo_92', 'demo_93', 'demo_94', 'demo_95', 'demo_96', 'demo_97', 'demo_98', 'demo_99', 'demo_100', 'demo_101', 'demo_102', 'demo_103', 'demo_104', 'demo_105', 'demo_106', 'demo_107', 'demo_108', 'demo_109', 'demo_110', 'demo_111', 'demo_112', 'demo_113', 'demo_114', 'demo_115', 'demo_116', 'demo_117', 'demo_118', 'demo_119', 'demo_120', 'demo_121', 'demo_122', 'demo_123', 'demo_124', 'demo_125', 'demo_126', 'demo_127', 'demo_128', 'demo_129', 'demo_130', 'demo_131', 'demo_132', 'demo_133', 'demo_134', 'demo_135', 'demo_136', 'demo_137', 'demo_138', 'demo_139', 'demo_140', 'demo_141', 'demo_142', 'demo_143', 'demo_144', 'demo_145', 'demo_146', 'demo_147', 'demo_148', 'demo_149', 'demo_150', 'demo_151', 'demo_152', 'demo_153', 'demo_154', 'demo_155', 'demo_156', 'demo_157', 'demo_158', 'demo_159', 'demo_160', 'demo_161', 'demo_162', 'demo_163', 'demo_164', 'demo_165', 'demo_166', 'demo_167', 'demo_168', 'demo_169', 'demo_170', 'demo_171', 'demo_172', 'demo_173', 'demo_174', 'demo_175', 'demo_176', 'demo_177', 'demo_178', 'demo_179', 'demo_180', 'demo_181', 'demo_182', 'demo_183', 'demo_184', 'demo_185', 'demo_186', 'demo_187', 'demo_188', 'demo_189', 'demo_190', 'demo_191', 'demo_192', 'demo_193', 'demo_194', 'demo_195', 'demo_196', 'demo_197', 'demo_198', 'demo_199']\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 110/110 [00:00<00:00, 408.88it/s]\n",
      "SequenceDataset: caching get_item calls...\n",
      "100%|██████████| 13008/13008 [00:01<00:00, 8549.72it/s]\n"
     ]
    }
   ],
   "source": [
    "demo_data = SequenceDataset(\n",
    "    hdf5_path = dataset_path,\n",
    "    obs_keys = config.all_modalities,\n",
    "    dataset_keys = (\"actions\",  \"rewards\",  \"dones\"),\n",
    "    demos_input_list = [f'demo_{i}' for i in range(90)],\n",
    "    load_next_obs = True,\n",
    "    frame_stack = 1,\n",
    "    seq_length = 1, # length-10 temporal sequences\n",
    "    pad_frame_stack = True,\n",
    "    pad_seq_length = True, # pad last obs per trajectory to ensure all sequences are sampled\n",
    "    get_pad_mask = False,\n",
    "    goal_mode = None,\n",
    "    hdf5_cache_mode = \"all\", # cache dataset in memory to avoid repeated file i/o\n",
    "    hdf5_use_swmr = True,\n",
    "    hdf5_normalize_obs = False,\n",
    "    filter_by_attribute = None, # can optionally provide a filter key here\n",
    ")\n",
    "\n",
    "# splitting training data and rest of the data\n",
    "dataset_test = demo_data + SequenceDataset(\n",
    "    hdf5_path = dataset_path,\n",
    "    obs_keys = config.all_modalities,\n",
    "    dataset_keys = (\"actions\", \"rewards\", \"dones\"),\n",
    "    demos_input_list = [f'demo_{i}' for i in range(90, 200)],\n",
    "    load_next_obs=True,\n",
    "    frame_stack = 1,\n",
    "    seq_length = 1, # length-10 temporal sequences\n",
    "    pad_frame_stack = True,\n",
    "    pad_seq_length = True, # pad last obs per trajectory to ensure all sequences are sampled\n",
    "    get_pad_mask = False,\n",
    "    goal_mode = None,\n",
    "    hdf5_cache_mode = \"all\", # cache dataset in memory to avoid repeated file i/o\n",
    "    hdf5_use_swmr = True,\n",
    "    hdf5_normalize_obs = False,\n",
    "    filter_by_attribute = None, # can optionally provide a filter key here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = list(range(len(demo_data)))\n",
    "left_idx = list(range(len(demo_data), len(dataset_test)))\n",
    "train_dset = [dataset_test[idx] for idx in train_idx]\n",
    "left_dset = [dataset_test[idx] for idx in left_idx]\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dset ,\n",
    "    sampler=None,       # no custom sampling logic (uniform sampling)\n",
    "    batch_size=100,     # batches of size 100\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True      # don't provide last batch in dataset pass if it's less than 100 in size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Ensemble Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 300\n",
    "kth = 5\n",
    "epoch_num = 100\n",
    "episodes_num = 50\n",
    "scss_rate = [-1, -1]\n",
    "training_loss = []\n",
    "single_scss_rate = []\n",
    "ensemble_num = 1\n",
    "big_iter = 0\n",
    "method = 'variance' # ['variance', 'max_error', 'min_error', 'random']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "There are 13008 samples left.\n",
      "training size is 10100\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 0\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 1\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 2\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "rollout: env=PickPlaceCan, horizon=400, use_goals=False, num_episodes=50\n",
      "100%|██████████| 50/50 [03:47<00:00,  4.54s/it]\n",
      "Succcess rate this round is 0.88\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "There are 12708 samples left.\n",
      "training size is 10400\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 0\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 1\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 2\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "rollout: env=PickPlaceCan, horizon=400, use_goals=False, num_episodes=50\n",
      "100%|██████████| 50/50 [04:49<00:00,  5.78s/it]\n",
      "Succcess rate this round is 0.74\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "There are 12408 samples left.\n",
      "training size is 10700\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 0\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 1\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 2\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "rollout: env=PickPlaceCan, horizon=400, use_goals=False, num_episodes=50\n",
      "100%|██████████| 50/50 [04:16<00:00,  5.13s/it]\n",
      "Succcess rate this round is 0.82\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "There are 12108 samples left.\n",
      "training size is 11000\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 0\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 1\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 2\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "rollout: env=PickPlaceCan, horizon=400, use_goals=False, num_episodes=50\n",
      "100%|██████████| 50/50 [04:20<00:00,  5.22s/it]\n",
      "Succcess rate this round is 0.78\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "There are 11808 samples left.\n",
      "training size is 11300\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 0\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 1\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 2\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "rollout: env=PickPlaceCan, horizon=400, use_goals=False, num_episodes=50\n",
      "100%|██████████| 50/50 [04:30<00:00,  5.41s/it]\n",
      "Succcess rate this round is 0.76\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs type: low_dim with modalities: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs type: image with modalities: []\n",
      "There are 11508 samples left.\n",
      "training size is 11600\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 0\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 1\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "\n",
      "----------------------------------------\n",
      "Model No. 2\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 0 -> (step = 100)\n",
      "Epoch 10 -> (step = 100)\n",
      "Epoch 20 -> (step = 100)\n",
      "Epoch 30 -> (step = 100)\n",
      "Epoch 40 -> (step = 100)\n",
      "Epoch 50 -> (step = 100)\n",
      "Epoch 60 -> (step = 100)\n",
      "Epoch 70 -> (step = 100)\n",
      "Epoch 80 -> (step = 100)\n",
      "Epoch 90 -> (step = 100)\n",
      "rollout: env=PickPlaceCan, horizon=400, use_goals=False, num_episodes=50\n",
      " 78%|███████▊  | 39/50 [03:22<00:51,  4.65s/it]"
     ]
    }
   ],
   "source": [
    "while (scss_rate[-1] < 0.96 and scss_rate[-2] < 0.9) and left_idx:\n",
    "    big_iter += 1\n",
    "\n",
    "    # Model ensemble, use the basic BC model as the basic version\n",
    "    model_ensemble = []\n",
    "    for _ in range(ensemble_num):\n",
    "        model_ensemble.append(get_example_model(dataset_path=dataset_path, device=device))\n",
    "\n",
    "    print('There are ' + str(len(left_idx)) + ' samples left.')\n",
    "    Ensemble_LOSS = []\n",
    "    count_model = 0\n",
    "\n",
    "    print('training size is ' + str(len(train_dataloader)*100))\n",
    "\n",
    "    data_loader_iter_lift = iter(train_dataloader)\n",
    "\n",
    "    for tmp_model_lift in model_ensemble:\n",
    "        print(\"\\n----------------------------------------\")\n",
    "        print(f'Model No. {count_model}')\n",
    "        print(\"----------------------------------------\\n\")\n",
    "        count_model += 1\n",
    "        # iterator for data_loader - it yields batches\n",
    "\n",
    "        # record losses\n",
    "        losses = []\n",
    "        for epoch in range(epoch_num):\n",
    "            for i_step in range(1, len(train_dset) // 100 + 1):\n",
    "                if i_step % 100 == 0 and epoch % 10 == 0:\n",
    "                    print(f'Epoch {epoch} -> (step = {i_step})')\n",
    "                try:\n",
    "                    batch = next(data_loader_iter_lift)\n",
    "                except StopIteration:\n",
    "                    # data loader ran out of batches - reset and yield first batch\n",
    "                    data_loader_iter_lift = iter(train_dataloader)\n",
    "                    batch = next(data_loader_iter_lift)\n",
    "\n",
    "                input_batch = tmp_model_lift.process_batch_for_training(batch)\n",
    "                info = tmp_model_lift.train_on_batch(batch=input_batch, epoch=50, validate=False)\n",
    "\n",
    "                # record loss\n",
    "                step_log = tmp_model_lift.log_info(info)\n",
    "                losses.append(step_log[\"Loss\"])\n",
    "        Ensemble_LOSS.append(losses.copy())\n",
    "    training_loss.append(Ensemble_LOSS.copy())\n",
    "\n",
    "    # evaluate the ensemble based policy\n",
    "    rollout_model_list = [RolloutPolicyEnsemble(model, obs_normalization_stats=None) for model in model_ensemble]\n",
    "\n",
    "    all_rollout_logs, video_paths, ob_ac_log = rollout_with_stats(\n",
    "        policy = rollout_model_list,\n",
    "        envs = envs,\n",
    "        horizon = config.experiment.rollout.horizon,\n",
    "        use_goals = config.use_goals,\n",
    "        num_episodes = episodes_num,\n",
    "        render = False,\n",
    "        video_dir = None, # (None, 'videos')\n",
    "        epoch = eval_epoch,\n",
    "        video_skip = config.experiment.get(\"video_skip\", 5),\n",
    "        terminate_on_success = config.experiment.rollout.terminate_on_success,\n",
    "    )\n",
    "\n",
    "    # save success rate after each round\n",
    "    scss_rate.append(all_rollout_logs[envname]['Success_Rate'])\n",
    "    print('Succcess rate this round is ' + str(all_rollout_logs[envname]['Success_Rate']))\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    # Active Sample Selection\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "\n",
    "    closest_idx_list = [] # Selected samples\n",
    "    if method == 'random':\n",
    "        # randomly select idx to add\n",
    "        closest_idx_list = random_index_select(left_idx, target)\n",
    "    elif method == 'variance':\n",
    "        # Computes prediction variance\n",
    "        ob_idx_dict, ob_array, ac_array = obdicts2array(ob_ac_log, [key for key in batch['obs'].keys()])\n",
    "        ac_var = np.var(np.var(ac_array, axis = 1), axis = 1)\n",
    "        ac_var_list = [(ac_var[i], i) for i in range(len(ac_var))]\n",
    "        ac_var_list.sort(key = lambda x:x[0])\n",
    "        # find data points closest to large variance locations\n",
    "        count = 0\n",
    "        left_obs_dict = dataset_obs(dataset_test, left_idx, [key for key in batch['obs'].keys()])\n",
    "        while len(closest_idx_list) < target:\n",
    "            _, tmp_idx = ac_var_list.pop(-1)\n",
    "            tmp_obs = ob_array[tmp_idx]\n",
    "            closest_idx = FindClosestIdx(left_obs_dict, tmp_obs, closest_idx_list, kth)\n",
    "            for j in closest_idx:\n",
    "                if j not in closest_idx_list:\n",
    "                    closest_idx_list.append(j[1])\n",
    "    elif method == 'max_error':\n",
    "        # select those with largest test error\n",
    "        prediction_error = prediction_error_compute(dataset_test, rollout_model_list)\n",
    "        closest_idx_list = error_idx_selection(prediction_error, target, train_idx, 'max')\n",
    "    elif method == 'min_error':\n",
    "        # select those with smallest test error\n",
    "        prediction_error = prediction_error_compute(dataset_test, rollout_model_list)\n",
    "        closest_idx_list = error_idx_selection(prediction_error, target, train_idx, 'min')\n",
    "\n",
    "    # update training dataset\n",
    "    UpdateTrain(train_idx, left_idx, closest_idx_list)\n",
    "\n",
    "    train_dset = [dataset_test[idx] for idx in train_idx]\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset = train_dset ,\n",
    "        sampler = None,       # no custom sampling logic (uniform sampling)\n",
    "        batch_size = 100,     # batches of size 100\n",
    "        shuffle = True,\n",
    "        num_workers = 0,\n",
    "        drop_last = True      # don't provide last batch in dataset pass if it's less than 100 in size\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test trained model on left data\n",
    "obs_name = ['object', 'robot0_joint_pos', 'robot0_joint_pos_cos', 'robot0_joint_pos_sin', \\\n",
    "           'robot0_joint_vel', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos', \\\n",
    "            'robot0_gripper_qvel']\n",
    "\n",
    "prediction_var = []\n",
    "prediction_mean = []\n",
    "labels = []\n",
    "prediction_error = []\n",
    "\n",
    "for case_i in range(len(dataset_test)):\n",
    "    tmp_ob_dict = {}\n",
    "    for key in obs_name:\n",
    "        if key in dataset_test[case_i]['obs']:\n",
    "            tmp_ob_dict[key] = dataset_test[case_i]['obs'][key]\n",
    "\n",
    "    # label the action\n",
    "    labels.append(dataset_test[case_i]['actions'])\n",
    "\n",
    "    tmp_prediction_list = []\n",
    "    for model_id in range(len(rollout_model_list)):\n",
    "        tmp_prediction_list.append(rollout_model_list[model_id](ob=tmp_ob_dict, goal=None))\n",
    "\n",
    "    tmp_prediction_list = np.array(tmp_prediction_list)\n",
    "    prediction_mean.append(tmp_prediction_list.mean(axis = 0))\n",
    "    prediction_var.append(tmp_prediction_list.var(axis = 0))\n",
    "    prediction_error.append(dataset_test[case_i]['actions'] - tmp_prediction_list.var(axis = 0))\n",
    "\n",
    "# error without normalizing\n",
    "error_rank = []\n",
    "var_rank = []\n",
    "for error_idx in range(len(prediction_error)):\n",
    "    error_rank.append((error_idx, np.linalg.norm(prediction_error[error_idx])))\n",
    "    var_rank.append((error_idx, np.mean(prediction_var[error_idx])))\n",
    "\n",
    "# rank according to error and var\n",
    "error_rank.sort(key = lambda x:x[1])\n",
    "error_idx_list = [ele[0] for ele in error_rank[-5000:]]\n",
    "var_rank.sort(key = lambda x:x[1])\n",
    "var_idx_list = [ele[0] for ele in var_rank[-5000:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results for Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Success rate\n",
    "with open(f\"{envname}_{demo_type}_scss_ensemble_{ensemble_num}_{method}.txt\", \"wb\") as fp:\n",
    "    pickle.dump(scss_rate, fp)\n",
    "\n",
    "# Training error\n",
    "with open(f\"{envname}_{demo_type}_trainloss_ensemble_{ensemble_num}_{method}.txt\", \"wb\") as fp:\n",
    "    pickle.dump(training_loss, fp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "166b7f579494df1ee1fb04c1427c8228073b8bc171a6a2f67de8d460966b4fbc"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('robot_learning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
